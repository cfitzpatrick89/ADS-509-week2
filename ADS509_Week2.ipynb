{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn.  If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd075bd4",
   "metadata": {},
   "source": [
    "### Conor Fitzpatrick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cfitzpatrick/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/cfitzpatrick/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/cfitzpatrick/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d800747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 433 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: emoji\n",
      "Successfully installed emoji-2.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/cfitzpatrick/Downloads/M1 Results\"\n",
    "\n",
    "\n",
    "twitter_folder = os.path.join(data_location, \"twitter/\")\n",
    "lyrics_folder = os.path.join(data_location, \"lyrics/\")\n",
    "\n",
    "# # These subfolders should still work if you correctly stored the \n",
    "# # data from the Module 1 assignment\n",
    "# twitter_folder = \"twitter/\"\n",
    "# lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33623357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "    number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "    and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "    of unique tokens, lexical diversity, and number of characters. \n",
    "    \"\"\"\n",
    "# Function for the descriptive stats\n",
    "def descriptive_stats(tokens_list, num_common=10):\n",
    "    # Flatten the list of lists into a single list of tokens\n",
    "    all_tokens = [token for sublist in tokens_list for token in sublist]\n",
    "    \n",
    "    # Calculate\n",
    "    num_tokens = len(all_tokens)\n",
    "    unique_tokens = set(all_tokens)\n",
    "    num_unique_tokens = len(unique_tokens)\n",
    "    num_characters = sum(len(token) for token in all_tokens)\n",
    "    lexical_diversity = num_unique_tokens / num_tokens if num_tokens else 0\n",
    "    \n",
    "    # Most common tokEns \n",
    "    most_common_tokens = Counter(all_tokens).most_common(num_common)\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"Number of tokens: {num_tokens}\")\n",
    "    print(f\"Number of unique tokens: {num_unique_tokens}\")\n",
    "    print(f\"Number of characters: {num_characters}\")\n",
    "    print(f\"Lexical diversity: {lexical_diversity:.2f}\")\n",
    "    print(f\"Most common tokens: {most_common_tokens}\")\n",
    "    \n",
    "    # Return the statistics as a list\n",
    "    return [num_tokens, num_unique_tokens, lexical_diversity, num_characters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The 13 most common tokens are: [('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1), ('with', 1), ('other', 1), ('in', 1), ('this', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: Assertion statements are beneficial for a number of reasons including detecting errors and logical issues, which can signify a bug. They are great for helping you debug problems and better document everything you are doing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "import os\n",
    "\n",
    "def load_lyrics_data(folder_path):\n",
    "    lyrics_data = {}\n",
    "    for artist_folder in os.listdir(folder_path):\n",
    "        artist_path = os.path.join(folder_path, artist_folder)\n",
    "        if os.path.isdir(artist_path):\n",
    "            lyrics_data[artist_folder] = {}\n",
    "            for file_name in os.listdir(artist_path):\n",
    "                song_title = file_name.split('.')[0]\n",
    "                with open(os.path.join(artist_path, file_name), 'r', encoding='utf-8') as file:\n",
    "                    lyrics_data[artist_folder][song_title] = file.read()\n",
    "    return lyrics_data\n",
    "\n",
    "lyrics_folder = \"/Users/cfitzpatrick/Downloads/M1 Results/lyrics/\"\n",
    "lyrics_data = load_lyrics_data(lyrics_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7e5036d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions for cher:\n",
      "ùôøùöõùöòùöûùöç ùöúùöûùöôùöôùöòùöõùöùùöéùöõ ùöòùöè ùöñùöéùöúùöúùö¢ ùöãùöûùöóùöú & ùöïùöéùöêùöêùöíùöóùöêùöú\n",
      "163„éùÔºèÊÑõ„Åã„Å£„Å∑üíú26Ê≠≥üçí Â∑•„ÄáÂ•Ω„Åç„Å™Â•≥„ÅÆÂ≠êüíì „Éï„Ç©„É≠„Éº„Åó„Å¶„Åè„Çå„Åü„ÇâDM„Åó„Åæ„Åôüß°\n",
      "csu\n",
      "Writer @Washinformer @SpelmanCollege alumna #DCnative Award-winning journalist & PR pro @IABC Fellow & Past Chair IG: bcscomm Email: wibsiler@gmail.com\n",
      "I‚Äôm unemployed and live with my parents. MOOPS!\n",
      "Descriptions for robynkonichiwa:\n",
      "\"I love chill\" ‚Ä¢Facebook / Instagram / SoundCloud: AngelxoArts‚Ä¢ https://t.co/447okKLKzA‚Ä¶\n",
      "books, movies, music, nature & TV shows. OG Sweetee since '12 thanks to YouTube recommending 'This Feeling' on my homepage ‚ô•Ô∏è\n",
      "(Am)auteur en herbe üå± - juriste en paille ü§° - Ami des fleurs üå∏üåà (sans la main verte) - music & books - #morecomingsoon... (si on en voit le bout)\n",
      "This Twitter profile is full of sarcasm and rants with the occasional moan, dont like me dont follow me! KLF Stan Account Aspiring Youth Council rep\n",
      "Flora Youssef - Blogger & Founder Posting review articles about the latest music üéµ https://t.co/dx4hoIom7T https://t.co/KsplT6mZzs\n"
     ]
    }
   ],
   "source": [
    "# Take out the descriptions and store in a dict \n",
    "def extract_descriptions(folder_path, file_names):\n",
    "    descriptions_dict = {}\n",
    "    for file_name in file_names:\n",
    "        artist = file_name.split('_')[0]  \n",
    "        descriptions = []\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            next(file)  # Skip the header line\n",
    "            for line in file:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) > 6:  # Ensure there's a description field\n",
    "                    descriptions.append(parts[6])\n",
    "        descriptions_dict[artist] = descriptions\n",
    "    return descriptions_dict\n",
    "\n",
    "# location of the Twitter data folder and the specific files\n",
    "twitter_folder = \"/Users/cfitzpatrick/Downloads/M1 Results/twitter/\"\n",
    "file_names = [\"cher_followers_data.txt\", \"robynkonichiwa_followers_data.txt\"]\n",
    "\n",
    "# Store ina dict\n",
    "descriptions_dict = extract_descriptions(twitter_folder, file_names)\n",
    "\n",
    "# Print the descriptions \n",
    "for artist, descriptions in descriptions_dict.items():\n",
    "    print(f\"Descriptions for {artist}:\")\n",
    "    for description in descriptions[:5]:  # Print the first 5 descriptions for inspection\n",
    "        print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b2f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1ea87921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cfitzpatrick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Punctuation and stopwords to use\n",
    "punctuation = set(string.punctuation)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we remove the punctuation\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(char for char in text if char not in punctuation)\n",
    "\n",
    "# split on whitespace to tokenize\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Eliminate stopwords\n",
    "def remove_stop(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Transformation pipeline\n",
    "def transform(text):\n",
    "    # Lowercase the entire text using casefold\n",
    "    text = text.casefold()\n",
    "    # Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    # Tokenize text using regex to split by word boundaries\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    # Remove stopwords\n",
    "    tokens = remove_stop(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a42d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9aca94",
   "metadata": {},
   "source": [
    "### Clean Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b47e9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine descriptions from both artists into a single list of dictionaries\n",
    "combined_descriptions = []\n",
    "for artist, descriptions in descriptions_dict.items():\n",
    "    for description in descriptions:\n",
    "        combined_descriptions.append({'artist': artist, 'description': description})\n",
    "\n",
    "# Convert the combined descriptions to a DataFrame\n",
    "descriptions_df = pd.DataFrame(combined_descriptions)\n",
    "\n",
    "# Apply the cleaning function to the 'description' column using transform\n",
    "descriptions_df['cleaned_description'] = descriptions_df['description'].apply(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0abdfa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>cleaned_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>ùôøùöõùöòùöûùöç ùöúùöûùöôùöôùöòùöõùöùùöéùöõ ùöòùöè ùöñùöéùöúùöúùö¢ ùöãùöûùöóùöú &amp; ùöïùöéùöêùöêùöíùöóùöêùöú</td>\n",
       "      <td>[ùôøùöõùöòùöûùöç, ùöúùöûùöôùöôùöòùöõùöùùöéùöõ, ùöòùöè, ùöñùöéùöúùöúùö¢, ùöãùöûùöóùöú, ùöïùöéùöêùöêùöíùöóùöêùöú]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>163„éùÔºèÊÑõ„Åã„Å£„Å∑üíú26Ê≠≥üçí Â∑•„ÄáÂ•Ω„Åç„Å™Â•≥„ÅÆÂ≠êüíì „Éï„Ç©„É≠„Éº„Åó„Å¶„Åè„Çå„Åü„ÇâDM„Åó„Åæ„Åôüß°</td>\n",
       "      <td>[163, ÊÑõ„Åã„Å£„Å∑, 26Ê≠≥, Â∑•„ÄáÂ•Ω„Åç„Å™Â•≥„ÅÆÂ≠ê, „Éï„Ç©„É≠„Éº„Åó„Å¶„Åè„Çå„Åü„Çâdm„Åó„Åæ„Åô]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>csu</td>\n",
       "      <td>[csu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>[writer, washinformer, spelmancollege, alumna,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>I‚Äôm unemployed and live with my parents. MOOPS!</td>\n",
       "      <td>[unemployed, live, parents, moops]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191118</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>singer of songs, type 1 diabetic, tired $jakel...</td>\n",
       "      <td>[singer, songs, type, 1, diabetic, tired, jake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191119</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>Dadx2/ Con-Arch/ Photographer/ DK #stemgr√∏nnes...</td>\n",
       "      <td>[dadx2, conarch, photographer, dk, stemgr√∏nnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191120</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>A year to change a life is still a year ‚ú®üòå</td>\n",
       "      <td>[year, change, life, still, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191121</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>Head of Consumer - Mango. Made in Melbourne. R...</td>\n",
       "      <td>[head, consumer, mango, made, melbourne, rambl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191122</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>Stand for what is right, even if you stand alone.</td>\n",
       "      <td>[stand, right, even, stand, alone]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2191123 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist                                        description  \\\n",
       "0                  cher           ùôøùöõùöòùöûùöç ùöúùöûùöôùöôùöòùöõùöùùöéùöõ ùöòùöè ùöñùöéùöúùöúùö¢ ùöãùöûùöóùöú & ùöïùöéùöêùöêùöíùöóùöêùöú   \n",
       "1                  cher          163„éùÔºèÊÑõ„Åã„Å£„Å∑üíú26Ê≠≥üçí Â∑•„ÄáÂ•Ω„Åç„Å™Â•≥„ÅÆÂ≠êüíì „Éï„Ç©„É≠„Éº„Åó„Å¶„Åè„Çå„Åü„ÇâDM„Åó„Åæ„Åôüß°   \n",
       "2                  cher                                                csu   \n",
       "3                  cher  Writer @Washinformer @SpelmanCollege alumna #D...   \n",
       "4                  cher    I‚Äôm unemployed and live with my parents. MOOPS!   \n",
       "...                 ...                                                ...   \n",
       "2191118  robynkonichiwa  singer of songs, type 1 diabetic, tired $jakel...   \n",
       "2191119  robynkonichiwa  Dadx2/ Con-Arch/ Photographer/ DK #stemgr√∏nnes...   \n",
       "2191120  robynkonichiwa         A year to change a life is still a year ‚ú®üòå   \n",
       "2191121  robynkonichiwa  Head of Consumer - Mango. Made in Melbourne. R...   \n",
       "2191122  robynkonichiwa  Stand for what is right, even if you stand alone.   \n",
       "\n",
       "                                       cleaned_description  \n",
       "0            [ùôøùöõùöòùöûùöç, ùöúùöûùöôùöôùöòùöõùöùùöéùöõ, ùöòùöè, ùöñùöéùöúùöúùö¢, ùöãùöûùöóùöú, ùöïùöéùöêùöêùöíùöóùöêùöú]  \n",
       "1              [163, ÊÑõ„Åã„Å£„Å∑, 26Ê≠≥, Â∑•„ÄáÂ•Ω„Åç„Å™Â•≥„ÅÆÂ≠ê, „Éï„Ç©„É≠„Éº„Åó„Å¶„Åè„Çå„Åü„Çâdm„Åó„Åæ„Åô]  \n",
       "2                                                    [csu]  \n",
       "3        [writer, washinformer, spelmancollege, alumna,...  \n",
       "4                       [unemployed, live, parents, moops]  \n",
       "...                                                    ...  \n",
       "2191118  [singer, songs, type, 1, diabetic, tired, jake...  \n",
       "2191119  [dadx2, conarch, photographer, dk, stemgr√∏nnes...  \n",
       "2191120                  [year, change, life, still, year]  \n",
       "2191121  [head, consumer, mango, made, melbourne, rambl...  \n",
       "2191122                 [stand, right, even, stand, alone]  \n",
       "\n",
       "[2191123 rows x 3 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f413e",
   "metadata": {},
   "source": [
    "### Clean Lyrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e8d7e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Transformation pipeline for lyrics\n",
    "def transform(text):\n",
    "    # Lowercase the entire text using casefold\n",
    "    text = text.casefold()\n",
    "    # Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    # Tokenize text using regex to split by word boundaries\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    # Remove stopwords\n",
    "    tokens = remove_stop(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "926d370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    artist                    song  \\\n",
      "0    robyn      robyn_includemeout   \n",
      "1    robyn          robyn_electric   \n",
      "2    robyn         robyn_beach2k20   \n",
      "3    robyn         robyn_lovekills   \n",
      "4    robyn       robyn_timemachine   \n",
      "..     ...                     ...   \n",
      "415   cher  cher_takeitfromtheboys   \n",
      "416   cher          cher_dreambaby   \n",
      "417   cher   cher_pleasedonttellme   \n",
      "418   cher     cher_ihopeyoufindit   \n",
      "419   cher       cher_classified1a   \n",
      "\n",
      "                                        cleaned_lyrics  \n",
      "0    [include, really, simple, single, pulse, repea...  \n",
      "1    [electric, electric, electric, natural, high, ...  \n",
      "2    [beach, 2k20, wanna, go, gonna, get, ok, call,...  \n",
      "3    [love, kills, youre, looking, love, get, heart...  \n",
      "4    [time, machine, hey, cant, believe, fit, threw...  \n",
      "..                                                 ...  \n",
      "415  [take, boys, scared, never, hard, keep, good, ...  \n",
      "416  [dream, baby, found, boy, hes, dream, baby, do...  \n",
      "417  [please, dont, tell, ya, shook, override, whyd...  \n",
      "418  [hope, find, clouds, arent, going, nowhere, da...  \n",
      "419  [classified, 1a, know, much, love, knew, surel...  \n",
      "\n",
      "[420 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to load lyrics data\n",
    "def load_lyrics_data(folder_path):\n",
    "    lyrics_data = {}\n",
    "    for artist_folder in os.listdir(folder_path):\n",
    "        artist_path = os.path.join(folder_path, artist_folder)\n",
    "        if os.path.isdir(artist_path):\n",
    "            lyrics_data[artist_folder] = {}\n",
    "            for file_name in os.listdir(artist_path):\n",
    "                song_title = file_name.split('.')[0]\n",
    "                with open(os.path.join(artist_path, file_name), 'r', encoding='utf-8') as file:\n",
    "                    lyrics_data[artist_folder][song_title] = file.read()\n",
    "    return lyrics_data\n",
    "\n",
    "#lyrics data location\n",
    "lyrics_folder = \"/Users/cfitzpatrick/Downloads/M1 Results/lyrics/\"\n",
    "lyrics_data = load_lyrics_data(lyrics_folder)\n",
    "\n",
    "# Flatten the lyrics data into a DataFrame \n",
    "lyrics_list = []\n",
    "for artist, songs in lyrics_data.items():\n",
    "    for song, lyrics in songs.items():\n",
    "        lyrics_list.append({'artist': artist, 'song': song, 'lyrics': lyrics})\n",
    "\n",
    "lyrics_df = pd.DataFrame(lyrics_list)\n",
    "\n",
    "# Clean the lyrics column\n",
    "lyrics_df['cleaned_lyrics'] = lyrics_df['lyrics'].apply(transform)\n",
    "\n",
    "# Print the cleaned lyrics for inspection\n",
    "print(lyrics_df[['artist', 'song', 'cleaned_lyrics']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a326af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics function\n",
    "def descriptive_stats(tokens_list, num_common=10):\n",
    "    # Flatten the list of lists into a single list of tokens\n",
    "    all_tokens = [token for sublist in tokens_list for token in sublist]\n",
    "    \n",
    "    # Calculate the stats we want\n",
    "    num_tokens = len(all_tokens)\n",
    "    unique_tokens = set(all_tokens)\n",
    "    num_unique_tokens = len(unique_tokens)\n",
    "    num_characters = sum(len(token) for token in all_tokens)\n",
    "    lexical_diversity = num_unique_tokens / num_tokens if num_tokens else 0\n",
    "    \n",
    "    # Get the most common tokens\n",
    "    most_common_tokens = Counter(all_tokens).most_common(num_common)\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"Number of tokens: {num_tokens}\")\n",
    "    print(f\"Number of unique tokens: {num_unique_tokens}\")\n",
    "    print(f\"Number of characters: {num_characters}\")\n",
    "    print(f\"Lexical diversity: {lexical_diversity:.2f}\")\n",
    "    print(f\"Most common tokens: {most_common_tokens}\")\n",
    "    \n",
    "    # Return the statistics as a list\n",
    "    return [num_tokens, num_unique_tokens, lexical_diversity, num_characters]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64cbc7",
   "metadata": {},
   "source": [
    "### For Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b00a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Data (cher) Descriptive Stats:\n",
      "Number of tokens: 15327345\n",
      "Number of unique tokens: 1224069\n",
      "Number of characters: 90934128\n",
      "Lexical diversity: 0.08\n",
      "Most common tokens: [('love', 217449), ('im', 139831), ('Ô∏è', 131875), ('life', 126418), ('music', 90024), ('de', 73084), ('follow', 63246), ('lover', 62278), ('like', 58737), ('mom', 55405)]\n",
      "[15327345, 1224069, 0.07986177645247758, 90934128]\n",
      "Twitter Data (robynkonichiwa) Descriptive Stats:\n",
      "Number of tokens: 1466362\n",
      "Number of unique tokens: 229673\n",
      "Number of characters: 8984097\n",
      "Lexical diversity: 0.16\n",
      "Most common tokens: [('music', 15345), ('love', 11820), ('Ô∏è', 9804), ('im', 9096), ('och', 7923), ('life', 7555), ('de', 6395), ('follow', 5650), ('like', 4956), ('lover', 4877)]\n",
      "[1466362, 229673, 0.1566277631307958, 8984097]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call descriptive_stats for the cleaned Twitter descriptions\n",
    "for artist in descriptions_df['artist'].unique():\n",
    "    artist_descriptions = descriptions_df[descriptions_df['artist'] == artist]['cleaned_description'].tolist()\n",
    "    print(f\"Twitter Data ({artist}) Descriptive Stats:\")\n",
    "    twitter_stats = descriptive_stats(artist_descriptions)\n",
    "    print(twitter_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d0e261",
   "metadata": {},
   "source": [
    "### For Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e8582967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics Data (robyn) Descriptive Stats:\n",
      "Number of tokens: 15276\n",
      "Number of unique tokens: 2167\n",
      "Number of characters: 73665\n",
      "Lexical diversity: 0.14\n",
      "Most common tokens: [('know', 308), ('dont', 301), ('im', 299), ('love', 275), ('got', 252), ('like', 232), ('baby', 222), ('youre', 169), ('never', 155), ('dance', 150)]\n",
      "[15276, 2167, 0.14185650693898927, 73665]\n",
      "Lyrics Data (cher) Descriptive Stats:\n",
      "Number of tokens: 35921\n",
      "Number of unique tokens: 3703\n",
      "Number of characters: 172588\n",
      "Lexical diversity: 0.10\n",
      "Most common tokens: [('love', 1004), ('im', 513), ('know', 486), ('dont', 440), ('youre', 333), ('time', 319), ('baby', 319), ('see', 308), ('oh', 306), ('one', 282)]\n",
      "[35921, 3703, 0.10308733053088723, 172588]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call descriptive_stats for the cleaned lyrics data\n",
    "for artist in lyrics_df['artist'].unique():\n",
    "    artist_lyrics = lyrics_df[lyrics_df['artist'] == artist]['cleaned_lyrics'].tolist()\n",
    "    print(f\"Lyrics Data ({artist}) Descriptive Stats:\")\n",
    "    lyrics_stats = descriptive_stats(artist_lyrics)\n",
    "    print(lyrics_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: I am sure if we included all the stop words, the top five words would probably be stop words like a, an, the, but, etc. These are so common in language that they would surely be the most prevelant. This would not give us much insight into the lyrics.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: I have to say, I am very unfamiliar with both Cher and Robyn, I am not sure I could name a song of either one. But it is interesting that Robyn tends to have a higher lyrical diversity. They are both pop singers, but obviously Cher is a bit more famous, maybe her songs tend to be simpler and less complex? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"‚ù§Ô∏è\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis üòÅ\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "269cd433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common emojis for cher: [('‚ù§', 79223), ('üåà', 47549), ('‚ô•', 33978), ('üè≥', 33412), ('‚ú®', 29468), ('üíô', 21379), ('üèª', 20930), ('üåä', 20223), ('‚úå', 16773), ('üíú', 16550)]\n",
      "Most common emojis for robynkonichiwa: [('‚ù§', 4783), ('üåà', 4685), ('üè≥', 3528), ('‚ô•', 3103), ('‚ú®', 2223), ('üèª', 1495), ('‚úå', 1189), ('üèº', 1139), ('‚ôÄ', 836), ('üíô', 809)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "import emoji\n",
    "from collections import Counter\n",
    "\n",
    "# Extract the emojis from text\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "# Ten most common\n",
    "def most_common_emojis(descriptions_df, num_common=10):\n",
    "    common_emojis = {}\n",
    "    for artist in descriptions_df['artist'].unique():\n",
    "        artist_descriptions = descriptions_df[descriptions_df['artist'] == artist]['description']\n",
    "        all_emojis = [emoji for desc in artist_descriptions for emoji in extract_emojis(desc)]\n",
    "        common_emojis[artist] = Counter(all_emojis).most_common(num_common)\n",
    "    return common_emojis\n",
    "\n",
    "# Get the ten most common emojis by artist\n",
    "common_emojis = most_common_emojis(descriptions_df)\n",
    "\n",
    "# Print the ten most common emojis by artist\n",
    "for artist, emojis in common_emojis.items():\n",
    "    print(f\"Most common emojis for {artist}: {emojis}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common hashtags for cher: [('#BLM', 9535), ('#Resist', 6036), ('#BlackLivesMatter', 4681), ('#resist', 3797), ('#FBR', 3239), ('#TheResistance', 2995), ('#blacklivesmatter', 2645), ('#1', 2627), ('#Resistance', 1919), ('#RESIST', 1823)]\n",
      "Most common hashtags for robynkonichiwa: [('#BlackLivesMatter', 337), ('#BLM', 306), ('#blacklivesmatter', 208), ('#1', 199), ('#music', 174), ('#Music', 113), ('#EDM', 86), ('#LGBTQ', 75), ('#TeamFollowBack', 59), ('#blm', 56)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Get hashtags from the text\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r'#\\w+', text)\n",
    "\n",
    "# Ten most common hashtags\n",
    "def most_common_hashtags(descriptions_df, num_common=10):\n",
    "    common_hashtags = {}\n",
    "    for artist in descriptions_df['artist'].unique():\n",
    "        artist_descriptions = descriptions_df[descriptions_df['artist'] == artist]['description']\n",
    "        all_hashtags = [hashtag for desc in artist_descriptions for hashtag in extract_hashtags(desc)]\n",
    "        common_hashtags[artist] = Counter(all_hashtags).most_common(num_common)\n",
    "    return common_hashtags\n",
    "\n",
    "# Get the ten most common hashtags by artist\n",
    "common_hashtags = most_common_hashtags(descriptions_df)\n",
    "\n",
    "# Print the ten most common hashtags by artist\n",
    "for artist, hashtags in common_hashtags.items():\n",
    "    print(f\"Most common hashtags for {artist}: {hashtags}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song titles for robyn: ['\"Include Me Out\"', '\"Electric\"', '\"Beach 2K20\"', '\"Love Kills\"', '\"Time Machine\"']\n",
      "Song titles for cher: ['\"Come And Stay With Me\"', '\"Pirate\"', '\"Stars\"', '\"These Days\"', '\"Love So High\"']\n"
     ]
    }
   ],
   "source": [
    "# Get song titles from the first line of lyrics\n",
    "def extract_song_titles(lyrics_data):\n",
    "    song_titles = {}\n",
    "    for artist, songs in lyrics_data.items():\n",
    "        titles = []\n",
    "        for song, lyrics in songs.items():\n",
    "            first_line = lyrics.split('\\n')[0].strip()\n",
    "            titles.append(first_line)\n",
    "        song_titles[artist] = titles\n",
    "    return song_titles\n",
    "\n",
    "# Extract song titles from the lyrics data\n",
    "song_titles = extract_song_titles(lyrics_data)\n",
    "\n",
    "# Print \n",
    "for artist, titles in song_titles.items():\n",
    "    print(f\"Song titles for {artist}: {titles[:5]}\")  # Print the first 5 titles for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c599cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in song titles for robyn: [('me', 11), ('you', 9), ('the', 8), ('my', 8), ('love', 6)]\n",
      "Most common words in song titles for cher: [('the', 54), ('you', 44), ('love', 38), ('i', 38), ('to', 28)]\n"
     ]
    }
   ],
   "source": [
    "# extract words from text\n",
    "def extract_words(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.casefold())\n",
    "\n",
    "# Five most common words in the song titles\n",
    "def most_common_words_in_titles(song_titles, num_common=5):\n",
    "    common_words = {}\n",
    "    for artist, titles in song_titles.items():\n",
    "        all_words = [word for title in titles for word in extract_words(title)]\n",
    "        common_words[artist] = Counter(all_words).most_common(num_common)\n",
    "    return common_words\n",
    "\n",
    "# Get the five most common words in song titles by artist\n",
    "common_words_in_titles = most_common_words_in_titles(song_titles)\n",
    "\n",
    "# Print them out\n",
    "for artist, words in common_words_in_titles.items():\n",
    "    print(f\"Most common words in song titles for {artist}: {words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    Axes(0.125,0.125;0.775x0.755)\n",
       "Artist 2    Axes(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: \\s+ matches whitespace characters, which can be spaces, tabs, newlines etc.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2294c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  artist                song  length\n",
      "0  robyn  robyn_includemeout     453\n",
      "1  robyn      robyn_electric     253\n",
      "2  robyn     robyn_beach2k20     291\n",
      "3  robyn     robyn_lovekills     413\n",
      "4  robyn   robyn_timemachine     233\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compile a regex pattern for collapsing whitespace\n",
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "# Function to tokenize lyrics by collapsing whitespace and splitting\n",
    "def tokenize_lyrics(lyric):\n",
    "    \"\"\"Strip and split on whitespace\"\"\"\n",
    "    return [item.lower() for item in collapse_whitespace.split(lyric.strip())]\n",
    "\n",
    "\n",
    "# Calculate song lengths in terms of number of tokens using the `tokenize_lyrics` function\n",
    "lyrics_df['length'] = lyrics_df['lyrics'].apply(lambda x: len(tokenize_lyrics(x)))\n",
    "\n",
    "# Print the song lengths DataFrame for inspection\n",
    "print(lyrics_df[['artist', 'song', 'length']].head())\n",
    "\n",
    "# Plot histogram of song lengths by artist\n",
    "lyrics_df.groupby('artist')['length'].plot(kind=\"hist\", density=True, alpha=0.5, legend=True)\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Histogram of Song Lengths by Artist')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
